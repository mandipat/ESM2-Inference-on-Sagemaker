{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##esm running"
      ],
      "metadata": {
        "id": "ZR6TikaC7ED-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtLnF9j3HvDc",
        "outputId": "700e6f12-267f-46cd-b80e-c60254d9747d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fair-esm\n",
            "  Downloading fair_esm-2.0.0-py3-none-any.whl.metadata (37 kB)\n",
            "Downloading fair_esm-2.0.0-py3-none-any.whl (93 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/93.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fair-esm\n",
            "Successfully installed fair-esm-2.0.0\n"
          ]
        }
      ],
      "source": [
        "pip install fair-esm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "rc6eNJaNGNBP",
        "outputId": "5ec9045a-f755-4fd6-8782-f380e6d622b8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/root/.cache/torch/hub/checkpoints/esm2_t12_35M_UR50D.pt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-ae19ea0d3162>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Now, load model and alphabet from local weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malphabet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mesm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model_and_alphabet_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/esm/pretrained.py\u001b[0m in \u001b[0;36mload_model_and_alphabet_local\u001b[0;34m(model_location)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;34m\"\"\"Load from local path. The regression weights need to be co-located\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mmodel_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0mmodel_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_location\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_has_regression_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1423\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1425\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1426\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1427\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/root/.cache/torch/hub/checkpoints/esm2_t12_35M_UR50D.pt'"
          ]
        }
      ],
      "source": [
        "# local_inference_test.py\n",
        "import torch\n",
        "import esm\n",
        "import os\n",
        "\n",
        "# Example: \"esm2_t6_8M_UR50D\", or whichever variant you have locally\n",
        "model_dir = \"/root/.cache/torch/hub/checkpoints/\"\n",
        "\n",
        "model_location = os.path.join(model_dir, \"esm2_t12_35M_UR50D.pt\")\n",
        "\n",
        "# Now, load model and alphabet from local weights\n",
        "model, alphabet = esm.pretrained.load_model_and_alphabet_local(model_location)\n",
        "\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# Tokenize example protein sequence (short example)\n",
        "sequence = \"MGYARVNAKTDVA...\"  # replace with your test sequence\n",
        "batch_converter = alphabet.get_batch_converter()\n",
        "\n",
        "batch_labels, batch_strs, batch_tokens = batch_converter(\n",
        "    [(\"protein1\", sequence)]\n",
        ")\n",
        "# Forward pass\n",
        "with torch.no_grad():\n",
        "    results = model(batch_tokens, repr_layers=[model.num_layers])\n",
        "\n",
        "# The hidden representation is typically in results[\"representations\"][layer_idx]\n",
        "embeddings = results[\"representations\"][model.num_layers]  # shape: [batch_size, seq_len, hidden_dim]\n",
        "\n",
        "print(\"Embeddings shape:\", embeddings.shape)\n",
        "print(\"Local inference test successful!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ZaY29sdGrqk"
      },
      "outputs": [],
      "source": [
        "# Load ESM-2 model\n",
        "batch_converter = alphabet.get_batch_converter()\n",
        "model.eval()  # disables dropout for deterministic results\n",
        "\n",
        "# Prepare data (first 2 sequences from ESMStructuralSplitDataset superfamily / 4)\n",
        "data = [\n",
        "    #(\"protein1\", \"MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG\"),\n",
        "    #(\"protein2\", \"KALTARQQEVFDLIRDHISQTGMPPTRAEIAQRLGFRSPNAAEEHLKALARKGVIEIVSGASRGIRLLQEE\"),\n",
        "    #(\"protein2 with mask\",\"KALTARQQEVFDLIRD<mask>ISQTGMPPTRAEIAQRLGFRSPNAAEEHLKALARKGVIEIVSGASRGIRLLQEE\"),\n",
        "    (\"protein3\",  \"<mask>\"),\n",
        "]\n",
        "batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
        "batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
        "\n",
        "# Extract per-residue representations (on CPU)\n",
        "with torch.no_grad():\n",
        "    results = model(batch_tokens, repr_layers=[12], return_contacts=True)\n",
        "token_representations = results[\"representations\"][12]\n",
        "\n",
        "# Generate per-sequence representations via averaging\n",
        "# NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.\n",
        "sequence_representations = []\n",
        "for i, tokens_len in enumerate(batch_lens):\n",
        "    sequence_representations.append(token_representations[i, 1 : tokens_len - 1].mean(0))\n",
        "\n",
        "print(token_representations.shape)\n",
        "#print(sequence_representations[1].shape)\n",
        "#print(sequence_representations[2].shape)\n",
        "\n",
        "# Look at the unsupervised self-attention map contact predictions\n",
        "# import matplotlib.pyplot as plt\n",
        "# for (_, seq), tokens_len, attention_contacts in zip(data, batch_lens, results[\"contacts\"]):\n",
        "#     plt.matshow(attention_contacts[: tokens_len, : tokens_len])\n",
        "#     plt.title(seq)\n",
        "#     plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ucxse9WWoqA8"
      },
      "outputs": [],
      "source": [
        "sequence=\"MVNA\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1NyyUflfp7Yj"
      },
      "outputs": [],
      "source": [
        "batch_converter = alphabet.get_batch_converter()\n",
        "\n",
        "# Convert sequence into tokens\n",
        "batch_converter = alphabet.get_batch_converter()\n",
        "\n",
        "# Convert sequence into tokens\n",
        "data = [(\"sequence\", sequence)]\n",
        "_, _, tokens = batch_converter(data)\n",
        "tokens = tokens.to(next(model.parameters()).device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(tokens)\n",
        "\n",
        "# Extract logits - access the logits tensor from the 'logits' key in the outputs dictionary\n",
        "logits = outputs['logits']  # Changed this line\n",
        "\n",
        "# Generate unmasked sequence (argmax over logits)\n",
        "predicted_tokens = logits.argmax(dim=-1)\n",
        "\n",
        "# **Get the integer representation of the predicted tokens**\n",
        "predicted_token_ids = predicted_tokens[0].cpu().numpy().tolist()\n",
        "\n",
        "# **Decode tokens using the alphabet's get_tok method**\n",
        "generated_sequence = \"\".join([alphabet.get_tok(token_id) for token_id in predicted_token_ids])\n",
        "\n",
        "# Extract embedding vector (e.g., first token embedding)\n",
        "embedding_vector = logits[0, 0,:].tolist()\n",
        "\n",
        "print({\"generated_sequence\": generated_sequence, \"embedding\": embedding_vector})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bvwz67JwqLZm"
      },
      "source": [
        "MGYARVNAKTDVA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sS96XFQgjFk0"
      },
      "source": [
        "##transformers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsxIPfHxjFce"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qw2JmpPbfy_M",
        "outputId": "0cbf2603-3921-42fd-c877-d5d585a0bb6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model and tokenizer saved to esm2-sagemaker\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Step 1: Load the model and tokenizer from Hugging Face Hub\n",
        "model_name = \"facebook/esm2_t6_8M_UR50D\"  # Replace with your model name\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
        "\n",
        "# Step 2: Save the model and tokenizer to a local directory\n",
        "save_dir = \"esm2-sagemaker\"  # Directory to save the model artifacts\n",
        "model.save_pretrained(save_dir)\n",
        "tokenizer.save_pretrained(save_dir)\n",
        "\n",
        "print(f\"Model and tokenizer saved to {save_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_afp3edgyXz"
      },
      "outputs": [],
      "source": [
        "model_dir=\"/content/esm2-sagemaker\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmgjd15Hgeh0"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForMaskedLM.from_pretrained(model_dir)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1PoyVv9hvWU",
        "outputId": "710a1ca0-3df6-4e8b-9e27-c0396a7476fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted sequence: <cls> M <eos>\n",
            "Predicted sequence with mask: <cls> M A K S <eos>\n"
          ]
        }
      ],
      "source": [
        "# prompt: using the model from esm2 on huggin face , geneate a sequnce of protein for masked entry . can i control the temperature of this\n",
        "\n",
        "import torch\n",
        "import esm\n",
        "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
        "\n",
        "# Load the ESM-2 model and tokenizer (assuming you've already downloaded it)\n",
        "model_name = \"facebook/esm2_t6_8M_UR50D\"  # Replace with your model name if different\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
        "\n",
        "# Prepare the input sequence with a masked token\n",
        "sequence = \"<mask>\"  # Or your sequence with a masked token\n",
        "inputs = tokenizer(sequence, return_tensors=\"pt\")\n",
        "\n",
        "# Generate sequence with temperature control\n",
        "with torch.no_grad():\n",
        "    # Remove logits_processor argument\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "\n",
        "    # Apply temperature scaling\n",
        "    temperature = 0.5  # Adjust temperature as needed\n",
        "    logits = logits / temperature\n",
        "\n",
        "    # Get predicted token IDs\n",
        "    predicted_token_ids = torch.argmax(logits, dim=-1)\n",
        "\n",
        "    # Decode the predictions\n",
        "    predicted_sequence = tokenizer.decode(predicted_token_ids[0])\n",
        "\n",
        "print(f\"Predicted sequence: {predicted_sequence}\")\n",
        "\n",
        "\n",
        "# Example usage with a different sequence:\n",
        "sequence_with_mask = \"MAKS\"\n",
        "inputs = tokenizer(sequence_with_mask, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Remove logits_processor argument\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    temperature = 1  # Example temperature\n",
        "    logits = logits / temperature\n",
        "    predicted_token_ids = torch.argmax(logits, dim=-1)\n",
        "    predicted_sequence = tokenizer.decode(predicted_token_ids[0])\n",
        "\n",
        "print(f\"Predicted sequence with mask: {predicted_sequence}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "rHYcBvybhwHE",
        "outputId": "1e694a72-23bf-464c-b496-22f006ad4b65"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'MaskedLMOutput' object has no attribute 'last_hidden_state'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-2db86bcf4264>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mpredict_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-2db86bcf4264>\u001b[0m in \u001b[0;36mpredict_fn\u001b[0;34m(sec, model, tokenizer)\u001b[0m\n\u001b[1;32m     28\u001b[0m     return {\n\u001b[1;32m     29\u001b[0m         \u001b[0;34m\"logits\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0;34m\"embeddings\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     }\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'MaskedLMOutput' object has no attribute 'last_hidden_state'"
          ]
        }
      ],
      "source": [
        "# prompt: using the model from esm2 on huggin face , geneate a sequnce of protein for masked entry . can i control the temperature of this\n",
        "\n",
        "import torch\n",
        "import esm\n",
        "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
        "\n",
        "# # Load the ESM-2 model and tokenizer (assuming you've already downloaded it)\n",
        "# model_name = \"facebook/esm2_t6_8M_UR50D\"  # Replace with your model name if different\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
        "\n",
        "# Prepare the input sequence with a masked token\n",
        "sequence = \"MKTIIALSYILCLVFAQKLPGNDNSTATLCLGHHAVPNGTIVKTITNDQIEVTNATELVQSSSTGKICNNPHR\"  # Or your sequence with a masked token\n",
        "def predict_fn(sec, model,tokenizer):\n",
        "    # Extract inputs\n",
        "    sequence = sec\n",
        "    tokenizer = tokenizer\n",
        "    model = model\n",
        "\n",
        "    # Tokenize input\n",
        "    inputs = tokenizer(sequence, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "    # Run inference\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Extract logits or embeddings\n",
        "    return {\n",
        "        \"logits\": outputs.logits.numpy().tolist(),\n",
        "        \"embeddings\": outputs.last_hidden_state.numpy().tolist()\n",
        "    }\n",
        "\n",
        "\n",
        "predict_fn(sequence,model,tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYJHK2L9l902",
        "outputId": "c7b36121-73e2-4861-e276-5572ab1a53e8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'logits_shape': torch.Size([1, 5, 33]),\n",
              " 'embeddings_shape': torch.Size([1, 5, 320])}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# prompt: using the model from esm2 on huggin face , geneate a sequnce of protein for masked entry . can i control the temperature of this\n",
        "\n",
        "import torch\n",
        "import esm\n",
        "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
        "\n",
        "# # Load the ESM-2 model and tokenizer (assuming you've already downloaded it)\n",
        "# model_name = \"facebook/esm2_t6_8M_UR50D\"  # Replace with your model name if different\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
        "\n",
        "# Prepare the input sequence with a masked token\n",
        "sequence = \"MKT\"  # Or your sequence with a masked token\n",
        "def predict_fn(sec, model,tokenizer):\n",
        "    # Extract inputs\n",
        "    sequence = sec\n",
        "    tokenizer = tokenizer\n",
        "    model = model\n",
        "\n",
        "    # Tokenize input\n",
        "    inputs = tokenizer(sequence, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "    # Run inference\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs, output_hidden_states=True) # Get hidden states\n",
        "\n",
        "    # Extract logits or embeddings\n",
        "    return {\n",
        "        #\"logits\": outputs.logits.numpy().tolist(),\n",
        "        #\"embeddings\": outputs.hidden_states[-1].numpy().tolist() # Get last hidden state\n",
        "        \"logits_shape\": outputs.logits.shape,\n",
        "        \"embeddings_shape\": outputs.hidden_states[-1].shape\n",
        "\n",
        "        #\"embeddings_shape\": outputs.hidden_states.shape\n",
        "    }\n",
        "\n",
        "\n",
        "predict_fn(sequence,model,tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Deployement\n"
      ],
      "metadata": {
        "id": "fesRgK7P67rF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_6fIFuzF2qw"
      },
      "outputs": [],
      "source": [
        "!pip install boto3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PcS8bmYFpCu"
      },
      "outputs": [],
      "source": [
        "import boto3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vCca3VoHbYv"
      },
      "outputs": [],
      "source": [
        "!pip install sagemaker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "NwilGXSSJCmW"
      },
      "outputs": [],
      "source": [
        "# prompt: set my aws crendentials locally\n",
        "\n",
        "import os\n",
        "os.environ['AWS_ACCESS_KEY_ID'] = 'AKIAYMKXKCQO6CLFRZ5U'\n",
        "os.environ['AWS_SECRET_ACCESS_KEY'] = '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'\n",
        "os.environ['AWS_DEFAULT_REGION'] = 'us-east-1'\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sagemaker\n",
        "from sagemaker.model import Model\n",
        "import boto3\n",
        "\n",
        "# AWS Configurations\n",
        "region = \"us-east-1\"\n",
        "role_arn = \"arn:aws:iam::576245601309:role/service-role/AmazonSageMaker-ExecutionRole-20250305T155111\"  # SageMaker IAM Role\n",
        "ecr_image_uri =\"576245601309.dkr.ecr.us-east-1.amazonaws.com/esm2-sagemaker:tasks\"  # choose  between v1 and v2\n",
        "\n",
        "sagemaker_session = sagemaker.Session()"
      ],
      "metadata": {
        "id": "J9lV7ln99HiI"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create SageMaker Model using Custom ECR Image\n",
        "model = Model(\n",
        "    image_uri=ecr_image_uri,\n",
        "    role=role_arn,\n",
        "    sagemaker_session=sagemaker_session\n",
        ")\n",
        "\n",
        "# Deploy the Model as an Endpoint\n",
        "predictor = model.deploy(\n",
        "    initial_instance_count=1,\n",
        "    instance_type=\"ml.m5.2xlarge\",\n",
        "    endpoint_name=\"esm2-final-endpoint-tasks\"\n",
        ")\n",
        "\n",
        "print(f\"Model Deployed! Endpoint Name: esm2-final-endpoint-latest \")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JR0ReTO39dSm",
        "outputId": "149d7b74-7ef5-48b5-db6e-d1769ac00ed0"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------!Model Deployed! Endpoint Name: esm2-final-endpoint-latest \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.delete_model()"
      ],
      "metadata": {
        "id": "usCxbEFOEyR7"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sagemaker_client = boto3.client('sagemaker')\n",
        "# List endpoints\n",
        "response = sagemaker_client.list_endpoints()\n",
        "endpoints = response['Endpoints']\n",
        "\n",
        "print(endpoints[0]['EndpointName'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hwwHhFClnNN",
        "outputId": "dbaed139-c17d-4c03-d571-7e9066a408bf"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "esm2-final-endpoint-tasks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sagemaker"
      ],
      "metadata": {
        "id": "Xy7A0ruAP0Ah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task = \"embedding\" # @param [\"embedding\",\"fill_mask\",\"get_shape\"]\n",
        "endpoint_name = \"esm2-final-endpoint-tasks\" # @param [\"esm2-final-endpoint-tasks\",\"esm2-final-endpoint-latest\"]\n",
        "sequence = \"MADEEKLPPGWEKRMSRSSGRVYYFNHITNASQWERPSGNREY\" # @param {\"type\":\"string\",\"placeholder\":\"MADEEKLPPGWEKRMSRSSGRVYYFNHITNASQWERPSGNRE\"}\n",
        "\n",
        "import boto3\n",
        "import json\n",
        "\n",
        "# Define the SageMaker runtime client\n",
        "runtime_client = boto3.client(\"sagemaker-runtime\")\n",
        "\n",
        "# Define the request body (Modify if necessary)\n",
        "payload = json.dumps({\"sequence\":sequence,\"task\": task})\n",
        "\n",
        "# Invoke the endpoint\n",
        "response = runtime_client.invoke_endpoint(\n",
        "    EndpointName=endpoint_name,\n",
        "    ContentType=\"application/json\",\n",
        "    Body=payload\n",
        ")\n",
        "\n",
        "# Read and parse the response\n",
        "result = json.loads(response[\"Body\"].read().decode())\n",
        "\n",
        "# Print the response\n",
        "print(\"Inference Response:\", result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkY_6TZH_Vc1",
        "outputId": "50d22dd9-97d1-4b74-9daa-610b36277ba9"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference Response: {'embedding': [-0.13313792645931244, -0.09015635401010513, 0.0798395648598671, -0.047529615461826324, -0.01607181318104267, -0.18073049187660217, -0.11182057857513428, 0.14660878479480743, 0.0012154745636507869, 0.05050351843237877, -0.06252329796552658, 0.05237150192260742, 0.11784368008375168, -0.026263583451509476, 0.0371893048286438, -0.031225521117448807, 0.1856650859117508, 0.017232326790690422, -0.06414731591939926, 0.06315524876117706, 0.008484629914164543, -0.13953809440135956, 0.04911363869905472, -0.3035200834274292, -0.06622686982154846, -0.1007676050066948, 0.0942196398973465, -0.142366424202919, -0.10009825229644775, -0.04067230969667435, 0.043027594685554504, 0.11605934798717499, -0.2317308634519577, -0.055339597165584564, -0.004689148627221584, -0.1766187995672226, 0.14008381962776184, -0.18860958516597748, -0.05876338854432106, -0.04004071652889252, -0.14778868854045868, -0.0712834894657135, 0.025524305179715157, -0.21233171224594116, -0.1539284735918045, 0.18176136910915375, 4.036805629730225, -0.05907070264220238, -0.024912789463996887, -0.09736417979001999, -0.015608887188136578, 0.0696321651339531, -0.06521975249052048, 0.15628987550735474, 0.27879488468170166, -0.05885189771652222, 0.09135234355926514, -0.021074406802654266, -0.16526630520820618, 0.029475631192326546, 0.16414113342761993, 0.01429148018360138, 0.06195712462067604, -0.09238751232624054, -0.28949663043022156, -0.14245586097240448, 0.15586088597774506, 0.07802177965641022, 0.24358367919921875, 0.12259354442358017, -0.01726972497999668, -0.14628520607948303, 0.30203762650489807, -0.10669208317995071, -0.10645706951618195, -0.07268782705068588, -0.2503083348274231, 0.23794613778591156, 0.09151660650968552, 0.010347764007747173, 0.22787244617938995, -0.1964564174413681, 0.06015274301171303, 0.10046883672475815, 0.18789160251617432, -0.06900911778211594, -0.0058911386877298355, -0.06686658412218094, 0.07021353393793106, 0.015560141764581203, 0.06637093424797058, 0.17903143167495728, -0.19910135865211487, 0.04262606427073479, -0.07062030583620071, 0.012376910075545311, 0.012098051607608795, 0.18773098289966583, 0.10168854147195816, 0.006573840510100126, 0.007025948725640774, 0.21290692687034607, 0.08355505764484406, 0.0005953561631031334, 0.025370195508003235, -0.06245378032326698, -0.28354063630104065, -0.04692135751247406, -0.11805666238069534, 0.11601219326257706, -0.04440326243638992, -0.061307601630687714, 0.04135558009147644, 0.10179483890533447, -0.0709177628159523, 0.03582489863038063, 0.11377262324094772, -0.06093912199139595, -0.24311739206314087, -0.168616384267807, 0.10995210707187653, 0.0504334457218647, -0.025430796667933464, -0.022842232137918472, -0.01272666547447443, -0.1776949018239975, -0.1139073371887207, -0.11720771342515945, 0.17465411126613617, 0.183911994099617, -0.24630406498908997, 0.1346794217824936, 0.006795420311391354, 0.07592374831438065, 0.046102020889520645, 0.003220060607418418, -0.0634806677699089, 0.10416851192712784, 0.14713437855243683, 0.03101957030594349, 0.12258481979370117, -0.12884196639060974, -0.0994836688041687, -0.15730351209640503, -0.09835144877433777, -0.18310023844242096, -0.20772743225097656, 0.008195935748517513, 0.12922361493110657, 0.027476442977786064, -0.07538668066263199, -0.08111392706632614, -0.059952154755592346, -0.009629432111978531, 0.16664685308933258, 0.08488725870847702, -0.10197870433330536, 0.027231337502598763, -0.06006953865289688, 0.036625638604164124, 0.10020361095666885, 0.07095569372177124, 0.15260468423366547, -0.12595729529857635, -0.027340061962604523, 0.10706011205911636, -0.05179125815629959, -0.21608616411685944, -0.2097233682870865, 0.30702853202819824, 0.09760989993810654, 0.08701423555612564, -0.21889792382717133, -0.2106887698173523, -0.18624833226203918, -0.1043209582567215, 0.03405645117163658, -0.05324432626366615, 0.036838073283433914, 0.0363670289516449, 0.07732661068439484, -0.13148124516010284, -0.016750136390328407, 0.0828465148806572, -0.05534195527434349, -0.07179384678602219, -0.1557590365409851, 0.15777428448200226, -0.25698360800743103, 0.07059080898761749, -0.1208534687757492, 0.031565118581056595, 0.03499904274940491, 0.16642282903194427, 0.006021648179739714, -0.010244566015899181, -0.2857137620449066, 0.2035266011953354, 0.09351684898138046, 0.12713629007339478, 0.05226745083928108, -0.06677152961492538, -0.23687957227230072, -0.08103813976049423, -0.0059196921065449715, 0.10892773419618607, -0.4694161117076874, 0.021323852241039276, 0.1069638654589653, -0.2898963391780853, 0.013028014451265335, 0.3136012852191925, 0.13940276205539703, 0.12346792966127396, 0.11704830825328827, -0.12922745943069458, -0.032243818044662476, -0.19912320375442505, 0.09377175569534302, 0.16346649825572968, -0.06476558744907379, -0.07318966090679169, 0.1730181872844696, -0.0026134266518056393, -0.04148976877331734, 0.06069207563996315, 0.040895283222198486, -0.20275592803955078, -0.02603909559547901, 0.07582345604896545, 0.060091279447078705, -0.09211481362581253, 0.04922151565551758, 0.13982237875461578, -0.0006631553987972438, -0.09324541687965393, -0.0014906944707036018, 0.1487182378768921, 0.06181726232171059, -0.11260416358709335, 0.055060531944036484, 0.16333261132240295, 0.01175638660788536, -0.07507540285587311, -0.055173929780721664, -0.06459961086511612, -0.11660314351320267, -0.11111558973789215, -0.15758869051933289, 0.14947408437728882, -0.019785238429903984, 0.012755915522575378, -0.0010011643171310425, 0.12121416628360748, 0.03341814503073692, -0.02258823625743389, -0.13159093260765076, -0.021692190319299698, -0.1339997947216034, 0.006678206846117973, 0.0397261418402195, -0.07581620663404465, 0.04103061929345131, 0.17350155115127563, 0.05743544548749924, 0.10413125157356262, -0.044600892812013626, 0.25912073254585266, -0.1474868804216385, 0.015842098742723465, 0.11467127501964569, -0.11637905240058899, -0.06879329681396484, 0.024141788482666016, 0.08635840564966202, -0.07492649555206299, -0.10120192170143127, 0.10417166352272034, 0.18201696872711182, -0.08081409335136414, -0.034911178052425385, -0.06313176453113556, 0.14947578310966492, 0.17070497572422028, 0.15112075209617615, 0.0112119410187006, 0.03350123018026352, 0.008575703017413616, 0.11243406683206558, -0.025936540216207504, -0.06326846033334732, 0.028747281059622765, 0.005562834441661835, -0.18086554110050201, -0.10721487551927567, -0.04233324155211449, 0.06429920345544815, -0.10615330934524536, -0.1017596423625946, -0.4608069062232971, -0.09728036820888519, 0.03252361714839935, 0.14796973764896393, -0.019264258444309235, -0.022081833332777023, -0.1179518923163414, 0.11481422930955887, -0.16278265416622162, -0.030912982299923897, 0.07194672524929047, 0.18945170938968658, 0.048041120171546936, 0.09052938967943192, -0.05459370091557503, -0.02291845716536045, 0.33350294828414917, 0.19005811214447021, 0.04030974581837654, -0.0632893294095993, 0.1573515385389328, -0.084382563829422, 0.26250454783439636, -0.042915694415569305, 0.26088443398475647, 0.027342183515429497, -0.05462808907032013, 0.04062097519636154, 0.02768104337155819, -0.11384312063455582, 0.15041130781173706, -0.0027222514618188143, 0.2680204212665558, 0.22802168130874634, 0.01116730272769928, 0.019635673612356186, 0.14379607141017914, 0.13043756783008575, -0.03962425887584686, 0.09252896159887314, 0.0018052930245175958, 0.09816092997789383, -0.008748335763812065, 0.19735626876354218, -0.1347828209400177, -0.04435399919748306, 0.0013583773979917169, 0.18702669441699982, -0.1123480498790741, 0.017387278378009796, -0.07328029721975327, 0.023573415353894234, 0.09510472416877747, 0.042295150458812714, -0.18004553020000458, -0.03109113872051239, 0.06717407703399658, -0.08278819918632507, 0.039142850786447525, 0.03617488965392113, -0.0033770203590393066, 0.07689531147480011, 0.1214638277888298, -0.01631099358201027, -0.18757225573062897, -0.042791277170181274, -0.09798085689544678, -0.11729303747415543, -0.03525380790233612, 0.020047591999173164, 0.007129486184567213, 0.194560244679451, -0.21971292793750763, -0.04871034622192383, -0.0701754167675972, 0.09920022636651993, -0.10293368250131607, -0.040856026113033295, -0.04374289512634277, 0.013942480087280273, -0.19839878380298615, 0.17549985647201538, 0.43749064207077026, 0.1481150984764099, 0.07605326175689697, -0.033967986702919006, -0.07836184650659561, -0.20421026647090912, 0.023327890783548355, -0.04910862818360329, 0.1614471673965454, 0.09471751004457474, -0.22242587804794312, 0.16259154677391052, -0.07602328062057495, -0.11493851244449615, 0.1496524065732956, -0.2220916599035263, 0.06154267117381096, 0.07506915926933289, -0.07943694293498993, -0.10180377215147018, 0.06905508041381836, 0.09387118369340897, 0.26123350858688354, -0.050826139748096466, -0.03688676282763481, 0.21654002368450165, -0.05987849459052086, 0.0721258893609047, 0.06100218743085861, 0.12982043623924255, 0.050510574132204056, -0.06299769878387451, -0.20772285759449005, 0.03416350856423378, 0.03147586062550545, 0.14609090983867645, -0.22456204891204834, -0.04483878239989281, -0.10601992905139923, 0.10601665824651718, 0.06791497021913528, -0.2778232991695404, 0.04385323449969292, -0.04959478601813316, 0.15961706638336182, 0.13424941897392273, 0.0030326927080750465, -0.23938845098018646, -0.10738695412874222, -0.015751700848340988, -0.2300969511270523, 0.14057137072086334, -0.042484212666749954, -0.141463041305542, -0.05593842640519142, 0.03175048530101776, 0.10512939840555191, 0.08988520503044128, -0.08101847767829895, 0.2344503253698349, -0.06805569678544998, -0.021874653175473213, -0.033884547650814056, 0.21345500648021698, -0.0034182320814579725, -0.07805470377206802, -0.03309241682291031, -0.0969923585653305, 0.014921324327588081, -0.10962928831577301, 0.06033426150679588, 0.10654309391975403, -0.03345879912376404, -0.0085002351552248, -0.07979992032051086, -0.28315529227256775, -0.2373059242963791, 0.06280399113893509, -0.1755308359861374, -0.5500388145446777, -0.07209905236959457, 0.0011001451639458537, -0.058279626071453094, -0.04117009788751602, -0.14875081181526184, -0.14815832674503326, -0.23343491554260254, 0.24827204644680023, 0.11122485995292664, -0.033143412321805954, 0.13558554649353027, 0.1209636703133583, -0.04797128215432167, -0.07495835423469543, -0.12511582672595978, 0.013750603422522545, 0.08660370856523514, 0.006228245794773102, -0.0394451729953289]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "pd.DataFrame(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "aCYTfWi-m3FM",
        "outputId": "d141f340-82b6-4a2f-e3b0-0f5fdec05238"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     embedding\n",
              "0    -0.133138\n",
              "1    -0.090156\n",
              "2     0.079840\n",
              "3    -0.047530\n",
              "4    -0.016072\n",
              "..         ...\n",
              "475  -0.125116\n",
              "476   0.013751\n",
              "477   0.086604\n",
              "478   0.006228\n",
              "479  -0.039445\n",
              "\n",
              "[480 rows x 1 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-914027fe-faf8-41dd-9a62-5cb6523fbf23\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>embedding</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.133138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.090156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.079840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.047530</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.016072</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>475</th>\n",
              "      <td>-0.125116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>476</th>\n",
              "      <td>0.013751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>477</th>\n",
              "      <td>0.086604</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>478</th>\n",
              "      <td>0.006228</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>479</th>\n",
              "      <td>-0.039445</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>480 rows × 1 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-914027fe-faf8-41dd-9a62-5cb6523fbf23')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-914027fe-faf8-41dd-9a62-5cb6523fbf23 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-914027fe-faf8-41dd-9a62-5cb6523fbf23');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-42809d56-a4f9-491d-95ff-c8c4d4e01378\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-42809d56-a4f9-491d-95ff-c8c4d4e01378')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-42809d56-a4f9-491d-95ff-c8c4d4e01378 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"pd\",\n  \"rows\": 480,\n  \"fields\": [\n    {\n      \"column\": \"embedding\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.224798816387475,\n        \"min\": -0.5500388145446777,\n        \"max\": 4.036805629730225,\n        \"num_unique_values\": 480,\n        \"samples\": [\n          -0.10669208317995071,\n          0.03416350856423378,\n          -0.11493851244449615\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests-aws4auth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHaV4zP7iMZn",
        "outputId": "f83c4a49-e978-409f-b710-2524eae0cc2f"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests-aws4auth in /usr/local/lib/python3.11/dist-packages (1.3.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from requests-aws4auth) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->requests-aws4auth) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->requests-aws4auth) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->requests-aws4auth) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->requests-aws4auth) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from requests_aws4auth import AWS4Auth\n",
        "import boto3\n",
        "import json\n",
        "\n",
        "endpoint_name = 'esm2-final-endpoint-latest'\n",
        "region = 'us-east-1'\n",
        "payload = {\"sequence\": \"MKTAYIAKQRQIFSRQLEERLGLIEVQ<mask><mask><mask>DGLAEHVSG<mask><mask><mask><mask>NITKSGII\",\"task\":\"fill_mask\"}\n",
        "\n",
        "# Get AWS credentials\n",
        "session = boto3.Session()\n",
        "credentials = session.get_credentials()\n",
        "\n",
        "auth = AWS4Auth(credentials.access_key, credentials.secret_key,\n",
        "                region, 'sagemaker', session_token=credentials.token)\n",
        "\n",
        "url = 'https://runtime.sagemaker.us-east-1.amazonaws.com/endpoints/esm2-final-endpoint-tasks/invocations'\n",
        "\n",
        "headers = {'Content-Type': 'application/json'}\n",
        "\n",
        "response = requests.post(url, auth=auth, json=payload, headers=headers)\n",
        "\n",
        "print(response.json())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8COcW4iTiKLI",
        "outputId": "f1d1b1b4-4ef9-4830-b432-478ca05fc6c2"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'filled_sequence': 'MKTAYIAKQRQIFSRQLEERLGLIEVQALLDGLAEHVSGLLLKNITKSGII'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## model testing\n"
      ],
      "metadata": {
        "id": "8zEFqWOCURgv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import logging\n",
        "from flask import Flask, request, jsonify\n",
        "import torch\n",
        "import esm\n",
        "\n",
        "# Configure logging\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.setLevel(logging.DEBUG)\n",
        "handler = logging.StreamHandler()\n",
        "formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\n",
        "handler.setFormatter(formatter)\n",
        "logger.addHandler(handler)\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "def model_fn():\n",
        "    \"\"\"\n",
        "    Load the ESM2 model and its alphabet from local files using\n",
        "    esm.pretrained.load_model_and_alphabet_local.\n",
        "    Ensure that the directory specified by model_location contains both:\n",
        "      - esm2_t12_35M_UR50D.pt\n",
        "      - esm2_t12_35M_UR50D-contact-regression.pt\n",
        "    Also, add argparse.Namespace as a safe global to avoid unpickling errors.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        model_dir = \"/root/.cache/torch/hub/checkpoints/\"\n",
        "        model_location = os.path.join(model_dir, \"esm2_t12_35M_UR50D.pt\")\n",
        "        import argparse\n",
        "        import torch.serialization\n",
        "        torch.serialization.add_safe_globals([argparse.Namespace])\n",
        "\n",
        "        model, alphabet = esm.pretrained.load_model_and_alphabet_local(model_location)\n",
        "        model.eval()\n",
        "        logger.info(\"ESM2 model loaded successfully from local weights\")\n",
        "        return model, alphabet\n",
        "    except Exception as e:\n",
        "        logger.error(\"Error loading local model: %s\", e)\n",
        "        raise e\n",
        "\n",
        "# Load the model and alphabet on startup.\n",
        "model_alphabet = model_fn()\n",
        "\n",
        "def input_fn(request_body, request_content_type='application/json'):\n",
        "    \"\"\"\n",
        "    Process the input JSON data.\n",
        "    Expected JSON format:\n",
        "      {\n",
        "         \"sequence\": \"YOUR_PROTEIN_SEQUENCE\",\n",
        "         \"task\": \"embedding\"  // or \"get_shape\" or \"fill_mask\"\n",
        "      }\n",
        "    If \"task\" is not provided, defaults to \"embedding\".\n",
        "    \"\"\"\n",
        "    if request_content_type == 'application/json':\n",
        "        try:\n",
        "            data = json.loads(request_body)\n",
        "            sequence = data.get('sequence', '')\n",
        "            if not sequence:\n",
        "                raise ValueError(\"No sequence provided\")\n",
        "            task = data.get('task', 'embedding')\n",
        "            logger.info(\"Received sequence (first 10 chars): %s\", sequence[:10])\n",
        "            logger.info(\"Task: %s\", task)\n",
        "            return {\"sequence\": sequence, \"task\": task}\n",
        "        except Exception as e:\n",
        "            logger.error(\"Error processing input: %s\", e)\n",
        "            raise e\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported content type: \" + request_content_type)\n",
        "\n",
        "def predict_fn(inputs, model_alphabet):\n",
        "    \"\"\"\n",
        "    Depending on the requested task, run one of the following:\n",
        "      - \"embedding\": Return the averaged embedding from layer 12.\n",
        "      - \"get_shape\": Return the shape of the token representations from layer 12.\n",
        "      - \"fill_mask\": Fill a masked token in the sequence.\n",
        "\n",
        "    For \"fill_mask\", the sequence must include a mask token.\n",
        "    The alphabet (esm.Alphabet) is used for tokenization and for retrieving the mask index.\n",
        "    \"\"\"\n",
        "    sequence = inputs[\"sequence\"]\n",
        "    task = inputs[\"task\"]\n",
        "    model, alphabet = model_alphabet\n",
        "\n",
        "    # Tokenize the input sequence.\n",
        "    batch_converter = alphabet.get_batch_converter()\n",
        "    data = [(\"protein\", sequence)]\n",
        "    batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
        "\n",
        "    if task == \"embedding\":\n",
        "        # Get token representations and average them (excluding special tokens).\n",
        "        with torch.no_grad():\n",
        "            results = model(batch_tokens, repr_layers=[12], return_contacts=False)\n",
        "        token_representations = results[\"representations\"][12]\n",
        "        embedding = token_representations[0, 1:-1].mean(dim=0)\n",
        "        logger.info(\"Embedding shape: %s\", embedding.shape)\n",
        "        return {\"embedding\": embedding.cpu().numpy().tolist()}\n",
        "\n",
        "    elif task == \"get_shape\":\n",
        "        # Return the shape of the token representations from layer 12.\n",
        "        with torch.no_grad():\n",
        "            results = model(batch_tokens, repr_layers=[12], return_contacts=False)\n",
        "        token_representations = results[\"representations\"][12]\n",
        "        shape = list(token_representations.shape)\n",
        "        logger.info(\"Token representations shape: %s\", shape)\n",
        "        return {\"shape\": shape}\n",
        "\n",
        "    elif task == \"fill_mask\":\n",
        "        # Get the mask token id from the alphabet.\n",
        "        mask_idx = alphabet.mask_idx\n",
        "        # Clone the token tensor to modify it.\n",
        "        tokens = batch_tokens[0].clone()\n",
        "        # Identify all positions where the token equals the mask token id.\n",
        "        masked_positions = (tokens == mask_idx).nonzero(as_tuple=True)[0]\n",
        "        if len(masked_positions) == 0:\n",
        "            raise ValueError(\"No mask token found in the sequence\")\n",
        "        # Run inference to obtain logits.\n",
        "        with torch.no_grad():\n",
        "            output = model(batch_tokens, repr_layers=[12], return_contacts=False)\n",
        "        if \"logits\" not in output:\n",
        "            raise ValueError(\"Model does not provide logits for fill_mask task\")\n",
        "        logits = output[\"logits\"]\n",
        "        # For each masked position, get the predicted token id.\n",
        "        predicted_token_ids = logits[0, masked_positions, :].argmax(dim=-1)\n",
        "        # Replace the mask token ids with the predicted token ids.\n",
        "        tokens[masked_positions] = predicted_token_ids\n",
        "        # Remove the special tokens (assume first and last tokens are special).\n",
        "        token_ids = tokens[1:-1].tolist()\n",
        "        # Convert token ids to their string representations.\n",
        "        tokens_str = [alphabet.get_tok(i) for i in token_ids]\n",
        "        # Combine tokens into a filled sequence.\n",
        "        filled_sequence = \"\".join(tokens_str)\n",
        "        logger.info(\"Filled sequence: %s\", filled_sequence)\n",
        "        return {\"filled_sequence\": filled_sequence}\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported task: {task}\")\n",
        "\n",
        "def output_fn(prediction, accept='application/json'):\n",
        "    \"\"\"\n",
        "    Format the prediction output as a JSON string.\n",
        "    \"\"\"\n",
        "    return json.dumps(prediction), accept\n",
        "\n",
        "@app.route('/ping', methods=['GET'])\n",
        "def ping():\n",
        "    \"\"\"\n",
        "    Health check endpoint to verify if the model is loaded.\n",
        "    \"\"\"\n",
        "    health = model_alphabet is not None\n",
        "    status = 200 if health else 404\n",
        "    return jsonify({'status': 'Healthy' if health else 'Unhealthy'}), status\n",
        "\n",
        "@app.route('/invocations', methods=['POST'])\n",
        "def invocations():\n",
        "    \"\"\"\n",
        "    Process incoming requests:\n",
        "      1. Parse input (including \"sequence\" and \"task\").\n",
        "      2. Run model inference based on the task.\n",
        "      3. Return the result as JSON.\n",
        "    \"\"\"\n",
        "    data = request.data.decode('utf-8')\n",
        "    content_type = request.content_type\n",
        "    try:\n",
        "        inputs = input_fn(data, content_type)\n",
        "        prediction = predict_fn(inputs, model_alphabet)\n",
        "        response, out_content_type = output_fn(prediction, content_type)\n",
        "        logger.info(\"Request processed successfully for task: %s\", inputs[\"task\"])\n",
        "        return response, 200, {'Content-Type': out_content_type}\n",
        "    except Exception as e:\n",
        "        logger.error(\"Error in /invocations: %s\", e)\n",
        "        return jsonify({'error': str(e)}), 500\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Load the model and alphabet\n",
        "    model_alphabet = model_fn()\n",
        "\n",
        "    # Define a sample protein sequence to test\n",
        "    test_sequence = \"MADEEKLPPGWEKRMSRSSGRVYYFNHITNASQWERPSGNRE\"\n",
        "    test_sequence='{\"sequence\": \"MADEEKLPPGWEKRMSRSSGRVYYFNHITNASQWERPSGNRE\",\"task\": \"get_shape\"}'\n",
        "    test_sequence='{\"sequence\": \"MKTAYIAKQRQIFSRQLEERLGLIEVQ<mask><mask><mask>DGLAEHVSG<mask><mask><mask><mask>NITKSGII\",\"task\": \"fill_mask\"}'\n",
        "    sequence = input_fn(test_sequence)\n",
        "\n",
        "    # Run inference to get the embedding\n",
        "    prediction = predict_fn(sequence, model_alphabet)\n",
        "\n",
        "    # Format and print the output\n",
        "    response = output_fn(prediction)\n",
        "    print(\"Prediction output:\")\n",
        "    print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rZLgcXDUUJ_",
        "outputId": "8e1732cf-6888-42bd-a86e-789b0b4f505a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-03-17 00:03:20,792 INFO ESM2 model loaded successfully from local weights\n",
            "2025-03-17 00:03:20,792 INFO ESM2 model loaded successfully from local weights\n",
            "2025-03-17 00:03:20,792 INFO ESM2 model loaded successfully from local weights\n",
            "2025-03-17 00:03:20,792 INFO ESM2 model loaded successfully from local weights\n",
            "2025-03-17 00:03:20,792 INFO ESM2 model loaded successfully from local weights\n",
            "2025-03-17 00:03:20,792 INFO ESM2 model loaded successfully from local weights\n",
            "2025-03-17 00:03:20,792 INFO ESM2 model loaded successfully from local weights\n",
            "INFO:__main__:ESM2 model loaded successfully from local weights\n",
            "2025-03-17 00:03:22,052 INFO ESM2 model loaded successfully from local weights\n",
            "2025-03-17 00:03:22,052 INFO ESM2 model loaded successfully from local weights\n",
            "2025-03-17 00:03:22,052 INFO ESM2 model loaded successfully from local weights\n",
            "2025-03-17 00:03:22,052 INFO ESM2 model loaded successfully from local weights\n",
            "2025-03-17 00:03:22,052 INFO ESM2 model loaded successfully from local weights\n",
            "2025-03-17 00:03:22,052 INFO ESM2 model loaded successfully from local weights\n",
            "2025-03-17 00:03:22,052 INFO ESM2 model loaded successfully from local weights\n",
            "INFO:__main__:ESM2 model loaded successfully from local weights\n",
            "2025-03-17 00:03:22,078 INFO Received sequence (first 10 chars): MKTAYIAKQR\n",
            "2025-03-17 00:03:22,078 INFO Received sequence (first 10 chars): MKTAYIAKQR\n",
            "2025-03-17 00:03:22,078 INFO Received sequence (first 10 chars): MKTAYIAKQR\n",
            "2025-03-17 00:03:22,078 INFO Received sequence (first 10 chars): MKTAYIAKQR\n",
            "2025-03-17 00:03:22,078 INFO Received sequence (first 10 chars): MKTAYIAKQR\n",
            "2025-03-17 00:03:22,078 INFO Received sequence (first 10 chars): MKTAYIAKQR\n",
            "2025-03-17 00:03:22,078 INFO Received sequence (first 10 chars): MKTAYIAKQR\n",
            "INFO:__main__:Received sequence (first 10 chars): MKTAYIAKQR\n",
            "2025-03-17 00:03:22,082 INFO Task: fill_mask\n",
            "2025-03-17 00:03:22,082 INFO Task: fill_mask\n",
            "2025-03-17 00:03:22,082 INFO Task: fill_mask\n",
            "2025-03-17 00:03:22,082 INFO Task: fill_mask\n",
            "2025-03-17 00:03:22,082 INFO Task: fill_mask\n",
            "2025-03-17 00:03:22,082 INFO Task: fill_mask\n",
            "2025-03-17 00:03:22,082 INFO Task: fill_mask\n",
            "INFO:__main__:Task: fill_mask\n",
            "2025-03-17 00:03:22,501 INFO Filled sequence: MKTAYIAKQRQIFSRQLEERLGLIEVQALLDGLAEHVSGLLLKNITKSGII\n",
            "2025-03-17 00:03:22,501 INFO Filled sequence: MKTAYIAKQRQIFSRQLEERLGLIEVQALLDGLAEHVSGLLLKNITKSGII\n",
            "2025-03-17 00:03:22,501 INFO Filled sequence: MKTAYIAKQRQIFSRQLEERLGLIEVQALLDGLAEHVSGLLLKNITKSGII\n",
            "2025-03-17 00:03:22,501 INFO Filled sequence: MKTAYIAKQRQIFSRQLEERLGLIEVQALLDGLAEHVSGLLLKNITKSGII\n",
            "2025-03-17 00:03:22,501 INFO Filled sequence: MKTAYIAKQRQIFSRQLEERLGLIEVQALLDGLAEHVSGLLLKNITKSGII\n",
            "2025-03-17 00:03:22,501 INFO Filled sequence: MKTAYIAKQRQIFSRQLEERLGLIEVQALLDGLAEHVSGLLLKNITKSGII\n",
            "2025-03-17 00:03:22,501 INFO Filled sequence: MKTAYIAKQRQIFSRQLEERLGLIEVQALLDGLAEHVSGLLLKNITKSGII\n",
            "INFO:__main__:Filled sequence: MKTAYIAKQRQIFSRQLEERLGLIEVQALLDGLAEHVSGLLLKNITKSGII\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction output:\n",
            "('{\"filled_sequence\": \"MKTAYIAKQRQIFSRQLEERLGLIEVQALLDGLAEHVSGLLLKNITKSGII\"}', 'application/json')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def model_fn(MODEL_PATH):\n",
        "    \"\"\"\n",
        "    Load the ESM2 model and its alphabet using the esm.pretrained API.\n",
        "    This method downloads the model if needed and sets it to evaluation mode.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        model_path = os.environ.get(MODEL_PATH, \"/root/.cache/torch/hub/checkpoints\")\n",
        "        model_location = os.path.join(model_path, \"esm2_t12_35M_UR50D.pt\")\n",
        "        if not os.path.exists(model_location):\n",
        "            logger.info(\"Downloading ESM2 model from AWS S3\")\n",
        "            model,alphabet= esm.pretrained.load_model_and_alphabet(\"esm2_t12_35M_UR50D\")\n",
        "            logger.info(\"ESM2 model downloaded successfully\")\n",
        "        else:\n",
        "            logger.info(\"Model already exists at %s\", model_location)\n",
        "            logger.info(\"Loading ESM2 model using esm.pretrained\")\n",
        "            try:\n",
        "                #import argparse\n",
        "                #import torch.serialization\n",
        "                #torch.serialization.add_safe_globals([argparse.Namespace])\n",
        "                model, alphabet = esm.pretrained.load_model_and_alphabet_local(model_location)\n",
        "                logger.info(\"ESM2 model with local model successfully\")\n",
        "            except Exception as e:\n",
        "                logger.error(\"Error loading model: %s\", e)\n",
        "                try:\n",
        "                    logging.info(\" Trying fallback model: esm2_t6_8M_UR50D\")\n",
        "                    model, alphabet = esm.pretrained.esm2_t6_8M_UR50D()\n",
        "                    model.eval()\n",
        "                    logging.info(\" Fallback model downloaded successfully!\")\n",
        "                except Exception as e:\n",
        "                    logging.error(\"Error loading fallback model: %s\", e)\n",
        "                    raise e\n",
        "\n",
        "        model.eval()\n",
        "        logger.info(\"ESM2 model loaded successfully\")\n",
        "        return model, alphabet\n",
        "    except Exception as e:\n",
        "        logger.error(\"Error loading model: %s\", e)\n",
        "\n",
        "\n",
        "# Load the model and alphabet on startup.\n",
        "model_alphabet = model_fn(\"/root/.cache/torch/hub/checkpoints/\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vy42h3djL7Yi",
        "outputId": "8af4bf62-90d3-4e0b-b296-08018c44c8e1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-03-16 23:48:41,377 INFO Model already exists at /root/.cache/torch/hub/checkpoints/esm2_t12_35M_UR50D.pt\n",
            "INFO:__main__:Model already exists at /root/.cache/torch/hub/checkpoints/esm2_t12_35M_UR50D.pt\n",
            "2025-03-16 23:48:41,381 INFO Loading ESM2 model using esm.pretrained\n",
            "INFO:__main__:Loading ESM2 model using esm.pretrained\n",
            "2025-03-16 23:48:41,385 ERROR Error loading model: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
            "\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
            "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
            "\tWeightsUnpickler error: Unsupported global: GLOBAL argparse.Namespace was not an allowed global by default. Please use `torch.serialization.add_safe_globals([Namespace])` or the `torch.serialization.safe_globals([Namespace])` context manager to allowlist this global if you trust this class/function.\n",
            "\n",
            "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n",
            "ERROR:__main__:Error loading model: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
            "\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
            "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
            "\tWeightsUnpickler error: Unsupported global: GLOBAL argparse.Namespace was not an allowed global by default. Please use `torch.serialization.add_safe_globals([Namespace])` or the `torch.serialization.safe_globals([Namespace])` context manager to allowlist this global if you trust this class/function.\n",
            "\n",
            "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n",
            "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t6_8M_UR50D.pt\" to /root/.cache/torch/hub/checkpoints/esm2_t6_8M_UR50D.pt\n",
            "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/regression/esm2_t6_8M_UR50D-contact-regression.pt\" to /root/.cache/torch/hub/checkpoints/esm2_t6_8M_UR50D-contact-regression.pt\n",
            "2025-03-16 23:48:42,362 INFO ESM2 model loaded successfully\n",
            "INFO:__main__:ESM2 model loaded successfully\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "sS96XFQgjFk0",
        "8zEFqWOCURgv"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}